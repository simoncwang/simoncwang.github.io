<!doctype html>
<html lang="en">
  <head>
    <!-- Required meta tags -->
    <meta charset="utf-8">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!-- Bootstrap CSS -->
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/css/bootstrap.min.css" integrity="sha384-MCw98/SFnGE8fJT3GXwEOngsV7Zt27NXFoaoApmYm81iuXoPkFOJwJ8ERdknLPMO" crossorigin="anonymous">
    <script src="https://code.jquery.com/jquery-3.3.1.slim.min.js" integrity="sha384-q8i/X+965DzO0rT7abK41JStQIAqVgRVzpbzo5smXKp4YfRvH+8abtTE1Pi6jizo" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/popper.js@1.14.3/dist/umd/popper.min.js" integrity="sha384-ZMP7rVo3mIykV+2+9J3UJ46jBk0WLaUAdn689aCwoqbBJiSnjAK/l8WvCWPIPm49" crossorigin="anonymous"></script>
    <script src="https://cdn.jsdelivr.net/npm/bootstrap@4.1.3/dist/js/bootstrap.min.js" integrity="sha384-ChfqqxuZUCnJSK3+MXmPNIyE6ZbWh2IMqE241rYiqJxyMiZ6OW/JmZQ5stwEULTy" crossorigin="anonymous"></script>
    <script src="https://kit.fontawesome.com/9804079aca.js" crossorigin="anonymous"></script>
    <script src="../javascript/util.js"></script>
    <script src="../javascript/theme_change.js"></script>
    <link rel = "stylesheet" href = "../style.css">
    <title>Projects</title>
  </head>

  <body>
    <!-- navbar section -->
    <nav id="topnav" class="navbar navbar-expand-lg navbar-light fixed-top">
      <button id="theme-toggle">
        <i id = "theme-icon" class="fa-regular fa-lightbulb"></i>
      </button>
      <div class="container">
        <a class="navbar-brand" href="../index.html">
          <i class="fa-solid fa-cow d-inline-block"></i>
          Simon Wang
        </a>
        <!-- <button class="navbar-toggler" type="button" data-toggle="collapse" data-target="#navbarNavAltMarkup" aria-controls="navbarNavAltMarkup" aria-expanded="false" aria-label="Toggle navigation">
          <span class="navbar-toggler-icon"></span>
        </button> -->
          <div class="navbar-nav">
            <a class="nav-item nav-link" href="../index.html">Home</a>
            <a class="nav-item nav-link active" href="./defaultproject.html">Projects</a>
            <a class="nav-item nav-link" href="../courses.html">Coursework</a>
            <a class="nav-item nav-link" href="../blog.html">Blog</a>
          </div>
      </div>
      
    </nav>

    <div class="project-page" data-theme-target>
      <!-- sidebar section -->
      <div id="mySidenav" class="sidenav" data-theme-target >
        <div class="collapse">
            <a href="javascript:void(0)" onclick="closeNav()"> &times;</a>
        </div>
    
        <div class="container-fluid" style="text-align:center">
            <!-- Sidebar Title -->
            <hr>

            <button class="section-header" onclick="toggleSection('projectSection')">School Projects</button>
            <div id="projectSection" class="section-content show-section">
                <a class="nav-item nav-link active" href="./defaultproject.html">MMO - MLLM Benchmarking</a>
                <a class="nav-item nav-link" href="./videogen.html">Consistent Video Generation</a>
                <a class="nav-item nav-link" href="./vrclass.html">XR Education</a>
                <a class="nav-item nav-link" href="./emd.html">Accessibility of Touch Screens for Elderly</a>
            </div>
    
            <button class="section-header" onclick="toggleSection('anotherSection')">Personal Projects</button>
            <div id="anotherSection" class="section-content">
              <a class="nav-item nav-link" href="./personalrag.html">RAG Quiz</a>
              <a class="nav-item nav-link" href="./artadvisor.html">AI Art Advisor</a>
            </div>
            
        </div>
      </div>
      
      <div id="expandBtn" class="expand">
        <a href="javascript:void(0)" onclick="openNav()"> &#9776;</a>
      </div>
    
      <!-- main content -->
      <div class="container" > 
        <div id = "myContent" class="content">
          <br>
          <h1 style="text-align: center;">MMO: An Investigation of Multi-modal Multi-agent
            Organization and Robust Benchmarking</h1>
          <h5 style="text-align: center; font-weight:lighter;">How to better evaluate benchmarks for Multimodal Large Language Models (MLLMs)</h5>
          
          <div class="links">
            <div>
              <i class="fa-brands fa-github-alt"></i> <a href="https://github.com/simoncwang/MMO" target="_blank">GitHub</a>
            </div>
            <div>
              <i class="fa-solid fa-book"></i> <a href="../documents/mmo.pdf" target="_blank">Technical Report</a>
            </div>
            
          </div>
          
          <br>
          <img src="../images/mllm.webp" class="blog-thumbnail rounded mx-auto d-block" style="width:50%">
          <center class="image-caption">AI generated using DALL-E</center>

          <hr>

          <h3>Introduction</h3>
          <p>
            With the increase of new multimodal large language models (MLLMs) being released with incredible results across various benchmarks, it has become more important than ever before to understand how we actually compare these amazing models. Most standard benchmarks used to demonstrate the capabilities of recent MLLMs have been curated datasets based on high level (college and beyond) questions, expert generated problems, and even screenshots of images from textbooks or webpages. Initially intending to develop a multi-agent system to leverage the power of small open-source MLLMs, through the course of my experimentation I now instead present an investigation into the shortcomings of popular benchmark evaluation, as well as a discussion of how it might be improved. In the process of evaluating benchmarks for various models as well as a multi-agent framework developed by myself, I also present a flexible system designed to test one or more MLLMs on common multimodal benchmarks.
          </p>


          <h3>What is an "MLLM"?</h3>
          <p>
            Most current MLLMs combine a vision encoder such as a vision transformer (ViT) with a standard pre-trained language model, thus incorporating both text and visual information into a single model. [PROVIDE MORE EXPLANATION AND EXAMPLES HERE]
          </p>


          <h3>How is benchmarking currently done?</h3>
          <p>
            Through the process of implementing my multi-agent system, I sought benchmarks to evaluate the quality of my models. Reading through reports and papers from popular new MLLMs such as QwenVL and InternVL amongst others, I chose the ScienceQA and MMMU benchmark datasets to dive deeper into. 
          </p>

          <p>
            In order to establish a baseline, I first attempted to look online for any publicly available
            evaluation code. Unfortunately, reading more into the Qwen and Intern papers as well as the official
            GitHub repositories, there was minimal code available to reproduce their results. However, on the
            ScienceQA GitHub I discovered code for evaluating GPT3 on the dataset, and decided to use this
            script as the base for my own code. Quickly, I discovered many challenges in implementing these
            evaluations. Under the hood, typical evaluation results are achieved through very extensive prompt
            engineering and brute force regular expression pattern matching. I also investigate other popular
            benchmarks such as GPQA (graduate level QA) and found very similar results.
          </p>


          <h3>The problem...</h3>
          <p>
            As mentioned earlier, I found many confusing and arbitrary lines of code throughout my research. For example, in the original ScienceQA evaluation code, the standard prompt actually includes an in-context example to help the model follow an expected format, such as: "output = f"Answer: The answer is {answer}."". Similarly, in the GPQA evaluation code, the model is simply asked nicely to return a format as follows: "After your reasoning, provide your final, single letter choice, formatted as "The answer is (X)"". <b>But, none of this is actually disclosed when model developers share their benchmark results (most of the time)!</b>
          </p>
          <p>
            After some trial and error to successfully run the original scripts, I achieved very low accuracy scores,
            especially for the open source models with Qwen2-VL-2B getting a score of 18.70%. Meanwhile,
            gpt-4o and gpt-4o-mini both achieved socres of over 85%. Upon closer inspection, the smaller
            MLLMs were producing many "invalid" results, with Qwen2-VL-2B producing over 700+. However,
            in many instances the results being produced are actually correct, but just not following the exact
            format expected by the evaluation code (e.g. "The answer is A"), instead returning a single letter or
            even a different format like "Choice A. In other benchmark repositories like GPQA, rather than deep approaches to improving the model being tested, high error rates are simply mitigated by providing a wide net to capture potential answer formats in the form of regular expression patterns. For example, here is the answer parsing code used in an actual GPQA evaluation example script:
          </p>
          <pre data-theme-target>
            def parse_sampled_answer(answer, answer_choice_tokens):
              """Copy pasted from GPQA repo + more patterns"""
              patterns = [
                r"answer is \((.)\)",
                r"Answer: \((.)\)",
                r"answer: \((.)\)",
                r"answer \((.)\)",
                r"answer is (\w)\.",
                r"Answer: (\w)\.",
                r"answer: (\w)\.",
                r"answer (\w)\.",
                r"answer is option \((.)\)",
                r"answer is Option \((.)\)",
                r"answer is option (\w)",
                r"answer is Option (\w)",
              ]
              for pattern in patterns:
                match = re.search(pattern, answer)
                if match and match.group(1) in answer_choice_tokens:
                  return match.group(1)
              return None
          </pre>

          <p>
            To illustrate this problem, I tested several models (gpt-4o, gpt-4o-mini, Qwen2-VL 2B Instruct, InternVL 2.5 2B, and Qwen2.5 1.5B Instruct) on a random 1000 samples (same seed) from the ScienceQA dataset. To show how arbitrary changes in prompting and parsing can drastically change benchmark results, the following graph shows accuracy across three strategies. The <i>base prompt</i> is very similar to the original ScienceQA prompt, simply asking the model to return their final answer choice as a single digit. The <i>ask twice</i> approach means just that...appending a prompt to the base prompt asking again "please return your choice in the following format: Answer: single digit". Finally, <i>single chars</i> is an additional regex pattern added to the end of the original list (which is the same as the GPQA parser expect modified to match single digits). Here, the last pattern allows for matching single digit answers, without the prompted "Answer: single digit" format.

          </p>
          <center><img src="../images/mmoprompting.png" style="width:60%"></center>

          <p>
            As you can see, trivial changes such as re-wording prompts, being more generous with answer matching, and other methods that I haven't explicitly tested like providing in-context examples can cause <b>seemingly random changes in accuracy.</b>
          </p>

          <p>
            Is this actually an honest way of "benchmarking" and comparing models? <b>How can we actually say one model is better than the other, if the prompting strategies and answer matching patterns used could be wildly different?</b> In my opinion, this approach to model evaluation is actually testing the ability of a team to prompt engineer, or the amount of resources a company has to run a benchmark long enough and enough times to get a better result.
          </p>

          <h3>An Observation and a Hypothesis</h3>
          <p>
            Throughout this month of research into these benchmarking methods, I have increasingly questioned what a "good" model actually means. In my opinion, it is highly likely that a big reason large models with many hundreds of billions of parameters like gpt-4o perform better on benchmarks is because <b>they just "listen" better</b> for lack of better terminology. Empirically, many of the smaller models I tested (all 2B parameters or under) would often answer the benchmark questions correctly, yet were very inconsistent in the wording and format of their responses. This leads to significantly deflated accuracy scores due to it, put simply, being hard to automate evaluating natural language answers. Which, is the point of large language models, right? More "capable" models like GPTs or Claude and larger Llama models (results not included in this project scope) seem to almost always return a response that follows what we ask for. OpenAI even has a new structured response beta that claims to guarantee 100% adherence to your specified format. 
          </p>

          <p>
            So, <b>could benchmarking language models in this way be fundamentally flawed?</b> Maybe not. In my opinion, at worst the discourse around benchmarking models is misleading. It is true that being better at following instructions is a highly desirable trait for a model, because, if we want use them for real-world applications we want them to listen to us of course. However, all the new papers claim that their models have improved [X subject] knowledge, or better logical reasoning, etc. etc.. This isn't quite accurate in my opinion, because as stated before, there should be more transparency about the methods used to actually achieve those numbers. What prompts did they use? How did they count correct answers? Without a central standardized body doing the actual testing, why <i>wouldn't</i> every company and researcher do everything they can to get the highest score? But does this really mean that the models are getting smarter...or are scores too influenced by the disparities in evaluation method?
          </p>

          <h3>My approach</h3>
          <p>
            How could we do better?
          </p>
          <p>
            As a small proof of concept, I developed a set of tools in Python to evaluate two benchmarks across many different types of models. I focused on working towards consistency of prompting and answer parsing, as well as easier control over datasets and models. Rather than writing a new unique script for every single model and dataset, my goal is to create one single system to evaluate all of the popular ones with a single tool. 
          </p>

          <p>
            This way, if the code and prompts used are the same (as similar as possible, sometimes models require different formats or data-types), the resulting accuracy scores will be fair for every model tested. Model A won't get a higher score than model B simply because the A had a longer regex pattern set!
          </p>

          <h3>MMO, a multimodal multi-agent organization system</h3>
          <p>
            Placeholder text.
          </p>

          <h3>Towards better benchmarks!</h3>
          <p>
            Placeholder text.
          </p>

          <h3>Future work</h3>
          <p>
            Placeholder text.
          </p>

        </div>
      </div>
    </div>
    

    
<!-- footer section -->
<div class="footer">
  <p>
    &copy; Simon Wang, 2024.
  </p>
  
  <p>
    Created with Bootstrap. &nbsp;
    <a href="https://getbootstrap.com/" target="_blank">
      <img src="../images/bootstrap-logo-black.svg" width="35" height="30" alt="">
    </a>
  </p>
</div>

</body>
</html>
